{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vavgRlH8SWd",
        "outputId": "0a58109a-328f-4f99-8028-e2c10ba08e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.0 textstat-0.7.4\n",
            "Collecting URLExtract\n",
            "  Downloading urlextract-1.9.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from URLExtract) (3.10)\n",
            "Collecting uritools (from URLExtract)\n",
            "  Downloading uritools-4.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from URLExtract) (4.3.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from URLExtract) (3.16.1)\n",
            "Downloading urlextract-1.9.0-py3-none-any.whl (21 kB)\n",
            "Downloading uritools-4.0.3-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: uritools, URLExtract\n",
            "Successfully installed URLExtract-1.9.0 uritools-4.0.3\n",
            "Collecting emojis\n",
            "  Downloading emojis-0.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading emojis-0.7.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: emojis\n",
            "Successfully installed emojis-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat\n",
        "!pip install URLExtract\n",
        "!pip install emojis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import glob, os\n",
        "from urlextract import URLExtract\n",
        "import textstat\n",
        "from nltk import ngrams\n",
        "import emojis\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Metrics:\n",
        "    # I split the text into tokens\n",
        "    def createToken(self, tweet):\n",
        "      # text=self.readf(tweet)\n",
        "        tokens = nltk.word_tokenize(tweet)  # divide il testo in token\n",
        "        return tokens\n",
        "\n",
        "    def createSentToken(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        tokens = nltk.sent_tokenize(tweet)  # dividei l testo in token di frasi\n",
        "        return tokens\n",
        "\n",
        "    # total number of characters\n",
        "    def totalCharacter(self, tweet):\n",
        "        # total=0\n",
        "        # tokens=self.createToken()\n",
        "        # sommo tutte le lunghezze dei token\n",
        "        # for i in range(len(tokens)):\n",
        "        #   total=total+len(tokens[i])\n",
        "        # return total\n",
        "        c = str(tweet)\n",
        "        return len(c)\n",
        "\n",
        "     # total number of uppercase characters\n",
        "    def CharacterUpper(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createToken(tweet)\n",
        "        # sommo tutte le lunghezze dei token\n",
        "        for i in range(len(tokens)):\n",
        "            tok = tokens[i]\n",
        "            for j in range(len(tok)):\n",
        "                if tok[j].isupper():\n",
        "                    total += 1\n",
        "        return total\n",
        "\n",
        "    # total number of lowercase characters\n",
        "    def CharacterLower(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createToken(tweet)\n",
        "        for i in range(len(tokens)):\n",
        "            tok = tokens[i]\n",
        "            for j in range(len(tok)):\n",
        "                if tok[j].islower():\n",
        "                    total += 1\n",
        "        return total\n",
        "\n",
        "    # number of numbers\n",
        "    def NumberCount(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createToken(tweet)\n",
        "        for i in range(len(tokens)):\n",
        "            tok = tokens[i]\n",
        "            if tok.isdecimal():\n",
        "                total += 1\n",
        "            else:\n",
        "                for j in range(len(tok)):\n",
        "                    if tok[j].isdecimal():\n",
        "                        total += 1\n",
        "        return total\n",
        "\n",
        "    # number of white spaces\n",
        "    def Whitespace(self, tweet):\n",
        "        total = 0\n",
        "        # text=self.readf(tweet)\n",
        "        for i in range(len(tweet)):\n",
        "            if tweet[i].isspace():\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "    # total number of special characters\n",
        "    def CharacterSpecial(self, tweet):\n",
        "        #  total=0\n",
        "        #  regex = re.compile('[@_#$%^&*/\\|~%]')\n",
        "        # tokens=self.createToken()\n",
        "        # for i in range(len(tokens)):\n",
        "        #   tok=tokens[i]\n",
        "        #  for j in range(len(tok)):\n",
        "        #     if regex.search(tok[j]):\n",
        "        #       total+=1\n",
        "        return self.totalCharacter(tweet) - self.CharacterUpper(tweet) - self.CharacterLower(tweet) - self.NumberCount(\n",
        "            tweet) - self.Whitespace(tweet) - self.NumberEmoji(tweet)\n",
        "\n",
        "    # number of words\n",
        "    def NumberWord(self, tweet):\n",
        "        total = 0\n",
        "        # text=self.readf(tweet)\n",
        "        # parses the string starting from the first character, when it finds a space it stops and inserts the portion of the string into a list (vector).\n",
        "        a = tweet.split(\" \")\n",
        "        # the list created could also contain words composed of an empty value. The for loop performs a check and counts only the values ​​that contain at least one character other than space.\n",
        "        for i in a:\n",
        "            if (i != \"\"):\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "    # number of emojis\n",
        "    def NumberEmoji(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return emojis.count(tweet)\n",
        "\n",
        "    # number of propositions\n",
        "    def NumberPhrase(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createSentToken(tweet)\n",
        "        total = len(tokens)\n",
        "        return total\n",
        "\n",
        "    # punctuation frequency\n",
        "    def PunctuationFrequency(self, tweet):\n",
        "        total = 0\n",
        "        # text=self.readf(tweet)\n",
        "        dist = nltk.FreqDist(tweet)\n",
        "        total = dist['.'] + dist[','] + dist[';'] + dist[':'] + dist['!'] + dist['?'] + dist['('] + dist[')'] + dist[\n",
        "            '<'] + dist['>'] + dist['-'] + dist['...'] + dist['\\\"'] + dist['\\'']\n",
        "        return total\n",
        "\n",
        "    # most common words\n",
        "    def CommonWord(self, tweet):\n",
        "        nopunt = []  # list without punctuation\n",
        "\n",
        "        lista = []\n",
        "        tokens = self.createToken(tweet)\n",
        "        #  print(tokens)\n",
        "        stop_words = nltk.corpus.stopwords.words()\n",
        "        regex = re.compile('[`.,’:!?()\\-<>\"\\'@_#\\$%\\^&\\*~%}{\\[\\]]')\n",
        "        # text=self.readf(tweet)\n",
        "        extractor = URLExtract()\n",
        "        urls = extractor.find_urls(tweet)\n",
        "        # print(nltk.word_tokenize (str(urls)))\n",
        "        em = emojis.get(tweet)\n",
        "        #  print(em)\n",
        "        # print(stop_words)\n",
        "        # for i in range(len(tokens)):\n",
        "        for i in tokens:\n",
        "\n",
        "            #  if tokens[i]!='.'and tokens[i]!=','and tokens[i]!=';'and tokens[i]!=':' and tokens[i]!='!' and tokens[i]!='?' and tokens[i]!='('  and tokens[i]!=')' and tokens[i]!='<' and tokens[i]!='>' and tokens[i]!='-' and tokens[i]!='...' and tokens[i]!='-' and tokens[i]!='\\\"' and tokens[i]!='\\'' and not regex.match(tokens[i]) and tokens[i] not in em and tokens[i] not in urls:\n",
        "            if (not regex.match(i) and not i in em and not i in str(urls)):\n",
        "                # if i!='.'and i!=','and i!=';'and i!=':' and i!='!' and i!='?' and i!='('  and i!=')' and i!='<' and i!='>' and i!='-' and i!='...' and i!='\"' and i!=\"'\" and i!=\"“\" and i!=\"’\"  and i!=\"''\" and not regex.match(i) and i not in em and not i in str(urls):\n",
        "                nopunt.append(i)\n",
        "\n",
        "        for i in nopunt:\n",
        "            if not i.lower() in stop_words:\n",
        "                lista.append(i)\n",
        "        # print(lista)\n",
        "        dist = nltk.FreqDist(lista)\n",
        "        com = dist.most_common(5)\n",
        "        return com\n",
        "\n",
        "    # number of uppercase words\n",
        "    def WordUpper(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createToken(tweet)\n",
        "        # sommo tutte le lunghezze dei token\n",
        "        for i in tokens:\n",
        "            if i.isupper():\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "    # number of lowercase words\n",
        "    def WordLower(self, tweet):\n",
        "        total = 0\n",
        "        tokens = self.createToken(tweet)\n",
        "        for i in tokens:\n",
        "            if i.islower():\n",
        "                total += 1\n",
        "        return total\n",
        "\n",
        "    # LengthOfWords\n",
        "    def AvgLenWord(self, tweet):\n",
        "        mid = 0\n",
        "        somma = 0\n",
        "        nopunt = []  # lista non contenente punteggiature\n",
        "        lista = []  # listafinale\n",
        "        # text=self.readf(tweet)\n",
        "        tokens = self.createToken(tweet)\n",
        "        # print(tokens)\n",
        "        regex = re.compile('[`.,’:!?()\\-<>\"\\'@_#\\$%\\^&\\*~%}{\\[\\]]')\n",
        "        extractor = URLExtract()\n",
        "        urls = extractor.find_urls(tweet)\n",
        "        em = emojis.get(tweet)\n",
        "        stop_words = nltk.corpus.stopwords.words()\n",
        "        # print(stop_words)\n",
        "        for i in tokens:\n",
        "            if (not regex.match(i) and not i in em and not i in str(urls)):\n",
        "                nopunt.append(i)\n",
        "        for i in nopunt:\n",
        "            if not i.lower() in stop_words:\n",
        "                lista.append(i)\n",
        "        # print(len(lista))\n",
        "        for i in range(len(lista)):\n",
        "            somma += len(lista[i])\n",
        "        dim = len(lista)\n",
        "        if dim == 0:\n",
        "            dim = 1\n",
        "        mid = somma / dim\n",
        "        return round(mid, 2)\n",
        "\n",
        "    # average sentence length\n",
        "    def AvgLenPhrase(self, tweet):\n",
        "        mid = 0\n",
        "        somma = 0\n",
        "        dimphr = []  # dimensioni frasi\n",
        "        phras = self.createSentToken(tweet)\n",
        "        totalPhras = len(phras)\n",
        "        for i in range(len(phras)):\n",
        "            total = 0\n",
        "            a = phras[i].split(\" \")\n",
        "            for i in a:\n",
        "                if (i != \"\"):\n",
        "                    total += 1\n",
        "            dimphr.append(total)\n",
        "        # print(dimphr)\n",
        "        for i in dimphr:\n",
        "            somma += i\n",
        "        # print(somma)\n",
        "        mid = somma / totalPhras\n",
        "        return round(mid, 2)\n",
        "\n",
        "    # number of bigrams\n",
        "    def generateBigrams(self, tweet):\n",
        "        tokens = self.createToken(tweet)\n",
        "        bigrams = list(ngrams(tokens, 2))  # bigrams\n",
        "        return len(bigrams)  # Return the count of bigrams\n",
        "\n",
        "    # number of trigrams\n",
        "    def generateTrigrams(self, tweet):\n",
        "        tokens = self.createToken(tweet)\n",
        "        trigrams = list(ngrams(tokens, 3))  # trigrams\n",
        "        return len(trigrams)  # Return the count of trigrams\n",
        "\n",
        "    # hapextokenratio\n",
        "    def HapaxTokenRatio(self, tweet):\n",
        "        tokens = self.createToken(tweet)\n",
        "        freq_dist = nltk.FreqDist(tokens)\n",
        "        hapax_count = len(freq_dist.hapaxes())\n",
        "        total_words = len(tokens)\n",
        "        if total_words == 0:\n",
        "            return 0.0\n",
        "        return round(hapax_count / total_words, 2)\n",
        "\n",
        "    # VocabularyRichness\n",
        "    def VocabularyWealth(self, tweet):\n",
        "      # text=self.readf()\n",
        "      nopunt = []  # lista non contenente punteggiature\n",
        "      lista = []\n",
        "      tokens = self.createToken(tweet)\n",
        "      regex = re.compile('[`.,’:!?()\\-<>\"\\'@_#\\$%\\^&\\*~%}{\\[\\]]')\n",
        "      em = emojis.get(tweet)\n",
        "\n",
        "      stop_words = nltk.corpus.stopwords.words()\n",
        "      # print(stop_words)\n",
        "      for i in tokens:\n",
        "          if (not regex.match(i) and not i in em):\n",
        "              nopunt.append(i)\n",
        "      # print(round(len(set(nopunt))/len(nopunt),2))\n",
        "      for i in nopunt:\n",
        "          if not i.lower() in stop_words:\n",
        "              lista.append(i)\n",
        "      # print(lista)\n",
        "      if len(lista) == 0:\n",
        "          return 0\n",
        "      return round(len(set(lista)) / len(lista), 2)\n",
        "\n",
        "    # number of url\n",
        "    def UrlNumber(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        extractor = URLExtract()\n",
        "        urls = extractor.find_urls(tweet)\n",
        "        return len(urls)\n",
        "\n",
        "    # Flesch Reading Ease formula\n",
        "    def Fre(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return round(textstat.flesch_reading_ease(tweet), 2)\n",
        "\n",
        "    # Flesch Kincaid Grade Level\n",
        "    def Fkgl(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return round(textstat.flesch_kincaid_grade(tweet), 2)\n",
        "\n",
        "    # Automated Readability Index (ARI)\n",
        "    def Ari(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return round(textstat.automated_readability_index(tweet), 2)\n",
        "\n",
        "    # Coleman Liau Index\n",
        "    def Cli(self, tweet):\n",
        "      # text=self.readf(tweet)\n",
        "      return round(textstat.coleman_liau_index(tweet), 2)\n",
        "\n",
        "   # Gunning Fog\n",
        "    def Gf(self, tweet):\n",
        "      #  text=self.readf(tweet)\n",
        "      return round(textstat.gunning_fog(tweet), 2)\n",
        "\n",
        "    # Dale-Chall Readability Score\n",
        "    def Dcr(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return textstat.dale_chall_readability_score(tweet)\n",
        "\n",
        "    # SMOG\n",
        "    def Smog(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return round(textstat.smog_index(tweet), 2)\n",
        "\n",
        "    # Linsear Write\n",
        "    def Lw(self, tweet):\n",
        "        # text=self.readf(tweet)\n",
        "        return round(textstat.linsear_write_formula(tweet), 2)\n",
        "\n",
        "# open a input csv file\n",
        "with open(r'/content/LLMs_Authors.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
        "    # reads the CSV file\n",
        "    df = pd.read_csv(file, engine='python', on_bad_lines='skip')\n",
        "\n",
        "inst = Metrics()\n",
        "\n",
        "# Apply the functions to the DataFrame\n",
        "df['NumberOfTotalCharacters'] = df['Tweet'].apply(inst.totalCharacter)\n",
        "df['NumberOfUppercaseCharacters'] = df['Tweet'].apply(inst.CharacterUpper)\n",
        "df['NumberOfLowercaseCharacters'] = df['Tweet'].apply(inst.CharacterLower)\n",
        "df['NumberOfSpecialCharacters'] = df['Tweet'].apply(inst.CharacterSpecial)\n",
        "df['NumberOfNumbers'] = df['Tweet'].apply(inst.NumberCount)\n",
        "df['NumberOfBlanks'] = df['Tweet'].apply(inst.Whitespace)\n",
        "df['NumberOfWords'] = df['Tweet'].apply(inst.NumberWord)\n",
        "df['AverageLengthOfWords'] = df['Tweet'].apply(inst.AvgLenWord)\n",
        "df['NumberOfPropositions'] = df['Tweet'].apply(inst.NumberPhrase)\n",
        "df['PropositionsLength'] = df['Tweet'].apply(inst.AvgLenPhrase)\n",
        "df['NumberOfPunctuationCharacters'] = df['Tweet'].apply(inst.PunctuationFrequency)\n",
        "df['NumberOfLowercaseWords'] = df['Tweet'].apply(inst.WordLower)\n",
        "df['NumberOfUppercaseWords'] = df['Tweet'].apply(inst.WordUpper)\n",
        "df['NumberOfBigrams'] = df['Tweet'].apply(inst.generateBigrams)\n",
        "df['NumberOfTrigrams'] = df['Tweet'].apply(inst.generateTrigrams)\n",
        "df['HapaxTokenRatio'] = df['Tweet'].apply(inst.HapaxTokenRatio)\n",
        "df['VocabularyRichness'] = df['Tweet'].apply(inst.VocabularyWealth)\n",
        "df['Flesch Kincaid Grade Level'] = df['Tweet'].apply(inst.Fkgl)\n",
        "df['Flesch Reading Ease formula'] = df['Tweet'].apply(inst.Fre)\n",
        "df['Dale Chall Readability'] = df['Tweet'].apply(inst.Dcr)\n",
        "df['Automated Readability Index'] = df['Tweet'].apply(inst.Ari)\n",
        "df['Coleman Liau Index'] = df['Tweet'].apply(inst.Cli)\n",
        "df['Gunning Fog'] = df['Tweet'].apply(inst.Gf)\n",
        "df['SMOG(Simple Measure of Gobbledygook)'] = df['Tweet'].apply(inst.Smog)\n",
        "df['Linsear Write'] = df['Tweet'].apply(inst.Lw)\n",
        "\n",
        "# Write dataframe to CSV file\n",
        "df.to_csv(r'/content/LLMs-Features.csv', index=False)\n",
        "print('All the Features are extracted Successful')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7iiwhzSJKvV",
        "outputId": "ba48abd1-f74d-4b06-ba51-ec9c10972bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h8gYNTMFr-of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfhhN2yIr-xg"
      }
    }
  ]
}